{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf9690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f57ef",
   "metadata": {},
   "source": [
    "# This is heavily based on the given solution for the class VAE exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e34bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "cuda2 = False # torch.cuda.is_available() check if a gpu is available\n",
    "batch_size2 = 128\n",
    "log_interval2 = 10\n",
    "epochs2 = 2 # 10\n",
    "latent_dim = 2\n",
    "\n",
    "torch.manual_seed(1) # args.seed\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda2 else \"cpu\") # args.cuda\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda2 else {} # args.cuda\n",
    "\n",
    "# Get train and test data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size2, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size2, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a4ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD # -ELBO\n",
    "\n",
    "def train(model, optimizer, epoch):\n",
    "    model.train() # so that everything was gradients and we can do backprop and so on...\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad() # \"reset\" gradients to 0 for text iteration\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward() # calc gradients\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step() # backpropagation\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): # no_grad turns of gradients...\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc1a = nn.Linear(400, 100)\n",
    "        self.fc21 = nn.Linear(100, latent_dim) # 20 -> 2\n",
    "        self.fc22 = nn.Linear(100, latent_dim) # 20 -> 2\n",
    "        self.fc3 = nn.Linear(latent_dim, 100) # 20 -> 2\n",
    "        self.fc3a = nn.Linear(100, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc1a(h1))\n",
    "        mu, logvar = self.fc21(h2), self.fc22(h2)\n",
    "        return torch.distributions.normal.Normal(mu, torch.exp(0.5*logvar)), mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc3a(h3))\n",
    "        return torch.sigmoid(self.fc4(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gauss, mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = gauss.rsample()\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f03f2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 188.5651\n",
      "====> Test set loss: 166.2209\n",
      "====> Epoch: 2 Average loss: 162.5133\n",
      "====> Test set loss: 159.2429\n"
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, epochs2 + 1):\n",
    "    train(model, optimizer, epoch)\n",
    "    test(model, epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, latent_dim).to(device) # 20 -> 2\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0956060",
   "metadata": {},
   "source": [
    "# Estimating model evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2aca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x0, _y0 = iter(test_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7393148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-165.8566,  -49.7395, -163.1648])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns batch of marginal likelihood estimates.\n",
    "def importance_sampling(\n",
    "    N, # number of samples to use for Monte Carlo estimator\n",
    "    x, # batch of images\n",
    "):\n",
    "    # Importance sampling for VAE. Oswin PML script, pp. 65.\n",
    "    # and Kingma and Welling. \"An Introduction to Variational Autoencoders\".\n",
    "    # Section 2.6.\n",
    "    with torch.no_grad():\n",
    "        # Draw from proposal distribution q(z|x) (decoder)\n",
    "        qz_x, mu, logvar = model.encode(x)\n",
    "        z = qz_x.rsample((N,))\n",
    "        \n",
    "        # Kingma and Welling, Section 1.7.3 for binary data:\n",
    "        #TODO; ours is not really binary, but neither is loss func?\n",
    "\n",
    "        # Note that the posterior has diagonal covariance\n",
    "        # by definition of the decoder in the VAE class above.\n",
    "        # Compute log q(z|x) = N(z;mu, diag(var))\n",
    "        log_qz_x = qz_x.log_prob(z).sum(-1)\n",
    "        #                          ^since diagonal variance => independent RVs\n",
    "        #                           q(z|x) = prod_i q(z_i|x)\n",
    "        #                           log q(z|x) = sum_i log q(z_i|x)\n",
    "        #                           See Kingma Section 2.5.\n",
    "        # Compute log p(z) = log N(z;0,I)\n",
    "        log_pz = torch.distributions.normal.Normal(\n",
    "            loc=torch.zeros_like(mu),\n",
    "            scale=torch.ones_like(logvar),\n",
    "        ).log_prob(z).sum(-1)\n",
    "        # Compute p(x|z) = sum_j log p(x_j|z) (by IID assumption)\n",
    "        #                = sum_j log Bernoulli(x_j; p_j)\n",
    "        #                  with p = decoder(z)\n",
    "        #                = sum_j x_j log p_j + (1-x_j) * log(1-p_j)\n",
    "        #                = -1*BCE of x and decoder(z)\n",
    "        log_px_z = -1*F.binary_cross_entropy(\n",
    "            model.decode(z),   # probabilities of Bernoulli R.V.s\n",
    "            x.repeat((N,1,1)), # match decoder(z) shape\n",
    "            reduction=\"none\",\n",
    "        ).sum(-1)\n",
    "        # ^sum over x feature dimension\n",
    "\n",
    "        # p(x) ~= 1/N sum_i^N p(x|z^(i))p(z^(i)) / q(z^(i)|x)\n",
    "        px = torch.mean(log_px_z + log_pz - log_qz_x, 0)\n",
    "        \n",
    "        #print(\"z\", z.shape)\n",
    "        #print(\"log_qz_x\", log_qz_x.shape)\n",
    "        #print(\"log_pz  \", log_pz.shape)\n",
    "        #print(\"log_px_z\", log_px_z.shape)\n",
    "        #print(\"px\", px.shape)\n",
    "        return px\n",
    "\n",
    "\n",
    "importance_sampling(10, x0.view(-1, 784))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54c13a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x) ~= -159.1983\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "px_estimates = []\n",
    "for (x, _y) in test_loader:\n",
    "    x = x.view(-1, 784).to(device)\n",
    "    px_estimates.append(importance_sampling(num_samples, x))\n",
    "\n",
    "pX = torch.cat(px_estimates).mean()\n",
    "print('p(x) ~= {:.4f}'.format(pX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e3915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d08e4f18bc401ba7a99c9eb02351518d315c4f9bdd03008a755c92232f7c69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
